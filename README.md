# The application implements a Retrieval-Augmented Generation (RAG) pipeline that processes various types of documents, extracts relevant information, and answers user queries using a Large Language Model (LLM). 

How to set up DeepSeek R1 on your local machine to securely query PDF documents using retrieval-augmented generation (RAG), from downloading and configuring the model with Ollama to building a Gradio-based web app that processes PDF files using LangChain and vector databases. Whether you’re on a Mac or Windows, this video covers data preprocessing, text embedding, and semantic search, giving you a comprehensive understanding of local AI-assisted document queries without any reliance on the cloud.

$ pip install langchain_community ollama gradio openai python-dotx python-pptx pytesseract

$ python simple_RAG_v3.py

![image](https://github.com/user-attachments/assets/b7aea105-4dad-4811-b569-6617fd7dfe0e)

![image](https://github.com/user-attachments/assets/3de49bea-f387-477b-983a-2660dff8ca05)

![image](https://github.com/user-attachments/assets/911b45a7-d8e3-460b-b131-ce61d55745d7)

project details can be found in this link: https://bit.ly/4hpF3du — You may need to sign up (for free!) to DataLab

https://youtu.be/hOsZzcMYMLI
